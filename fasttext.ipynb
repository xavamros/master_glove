{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-addons\n",
        "!pip install -q tensorflow\n",
        "!pip install -q datasets\n",
        "!pip install -q gensim\n",
        "!pip install sklearn_crfsuite\n",
        "!pip install -q fasttext\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "#!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.bin.gz\n",
        "!gunzip cc.en.300.bin.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4z7y56-7Er5",
        "outputId": "84daec16-5136-45d7-fbae-9ff5dc259286"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sklearn_crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn_crfsuite)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (4.65.0)\n",
            "Installing collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.9 sklearn_crfsuite-0.3.6\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--2023-08-08 19:26:35--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.162.83, 108.157.162.108, 108.157.162.120, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.162.83|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4503593528 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.en.300.bin.gz’\n",
            "\n",
            "cc.en.300.bin.gz    100%[===================>]   4.19G  79.3MB/s    in 44s     \n",
            "\n",
            "2023-08-08 19:27:19 (97.3 MB/s) - ‘cc.en.300.bin.gz’ saved [4503593528/4503593528]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import datasets\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader as gensim_api\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C6jFgQm7Gmd",
        "outputId": "71efbae2-0041-4de6-c8ff-1fc42f7b4087"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data from the url\n",
        "url = \"https://drive.google.com/uc?id=1LYi4J9yquBzyVE1op9_uyFLPdo9-jq8t\"\n",
        "train_data = pd.read_csv(url, delimiter='\\t', header=None, names=['word', 'tag'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "#msk = np.random.rand(len(data)) < 0.8\n",
        "#train_data = data[msk]\n",
        "\n",
        "# Convert the words and tags into numerical representations\n",
        "word2idx = {w: i+1 for i, w in enumerate(set(train_data['word']))}\n",
        "tag2idx = {'O': 0, 'B_PRODUCT': 1, 'I_PRODUCT': 2}\n",
        "train_data['word_idx'] = train_data['word'].map(word2idx)\n",
        "train_data['tag_idx'] = train_data['tag'].map(tag2idx)\n",
        "\n",
        "# Create tokens, id, and ner_tags list from the training data\n",
        "tokens = []\n",
        "ids = []\n",
        "ner_tags = []\n",
        "temp_token_list = []\n",
        "temp_id_list = []\n",
        "temp_ner_tag_list = []\n",
        "\n",
        "# Iterate through the rows of the training data\n",
        "for index, row in train_data.iterrows():\n",
        "    # Check if the word column starts with \"|\"\n",
        "    if row['word'][0] == '|':\n",
        "        # Add the temporary lists to the main lists\n",
        "        if len(temp_token_list)>0:\n",
        "            tokens.append(temp_token_list)\n",
        "            ids.append(temp_id_list)\n",
        "            ner_tags.append(temp_ner_tag_list)\n",
        "        # Clear the temporary lists\n",
        "        temp_token_list = []\n",
        "        temp_id_list = []\n",
        "        temp_ner_tag_list = []\n",
        "    else:\n",
        "        # Add the word, tag, and tag_idx to the temporary lists\n",
        "        temp_token_list.append(row['word'])\n",
        "        temp_id_list.append(tag2idx[row['tag']])\n",
        "        temp_ner_tag_list.append(row['tag'])\n",
        "\n",
        "# Add the last example to the main lists\n",
        "if len(temp_token_list)>0:\n",
        "    tokens.append(temp_token_list)\n",
        "    ids.append(temp_id_list)\n",
        "    ner_tags.append(temp_ner_tag_list)\n",
        "\n",
        "# Split the dataset into train and test datasets\n",
        "tokens_train, tokens_test, ids_train, ids_test, ner_tags_train, ner_tags_test = train_test_split(\n",
        "    tokens, ids, ner_tags, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a dictionary with 'tokens', 'id', and 'ner_tags' keys for train and test datasets\n",
        "train_dict = {'tokens': tokens_train, 'id': ids_train, 'ner_tags': ner_tags_train}\n",
        "test_dict = {'tokens': tokens_test, 'id': ids_test, 'ner_tags': ner_tags_test}\n",
        "\n",
        "# Create Dataset objects with the train_dict and test_dict dictionaries\n",
        "train_dataset = Dataset.from_dict(train_dict)\n",
        "test_data = Dataset.from_dict(test_dict)"
      ],
      "metadata": {
        "id": "XT1pQROH7Kqh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in train_dataset:\n",
        "  sample_tokens = item['tokens']\n",
        "  sample_tag_ids = item[\"id\"]\n",
        "  print(sample_tokens)\n",
        "  print(sample_tag_ids)\n",
        "  break\n",
        "\n",
        "raw_tags = ['O', 'B_PRODUCT', 'I_PRODUCT']\n",
        "print(raw_tags)\n",
        "print(type(raw_tags))\n",
        "\n",
        "sample_tags = [raw_tags[i] for i in sample_tag_ids]\n",
        "\n",
        "print(sample_tokens)\n",
        "print(sample_tags)\n",
        "\n",
        "tags = ['<PAD>'] + raw_tags\n",
        "print(tags)\n",
        "\n",
        "TAG_SIZE = len(tags)\n",
        "VOCAB_SIZE = 20000\n",
        "EMBEDDING_DIM = 300  # set the embedding dimension to 100\n",
        "\n",
        "train_tokens = tf.ragged.constant(train_dataset[\"tokens\"])\n",
        "train_tokens = tf.map_fn(tf.strings.lower, train_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oNXs6ZE7UBZ",
        "outputId": "20db487d-b141-44e5-dc91-2e32eec6dd2a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CUTTING', 'SET', 'HSS', 'PIE']\n",
            "[1, 2, 2, 2]\n",
            "['O', 'B_PRODUCT', 'I_PRODUCT']\n",
            "<class 'list'>\n",
            "['CUTTING', 'SET', 'HSS', 'PIE']\n",
            "['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT']\n",
            "['<PAD>', 'O', 'B_PRODUCT', 'I_PRODUCT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "embedding_path = \"cc.en.300.bin\"\n",
        "model_ft = fasttext.load_model(embedding_path)\n",
        "\n",
        "lookup_layer = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=list(model_ft.words)[:20000], mask_token=None\n",
        ")\n",
        "embedding_matrix = np.zeros((lookup_layer.vocabulary_size(), EMBEDDING_DIM))\n",
        "for i, word in enumerate(lookup_layer.get_vocabulary()):\n",
        "    if word in model_ft:\n",
        "        embedding_matrix[i] = model_ft.get_word_vector(word)\n",
        "\n",
        "embedding_layer = tf.keras.layers.Embedding(\n",
        "    lookup_layer.vocabulary_size(), EMBEDDING_DIM, mask_zero=True,\n",
        "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False  # set trainable to False to use the pre-trained embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "L_rasrvY7c8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "343c606b-dd5d-43c3-f0ef-872892025010"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_generator(dataset):\n",
        "  def data_generator():\n",
        "    for item in dataset:\n",
        "      yield item['tokens'], item['id']\n",
        "\n",
        "  return data_generator\n",
        "\n",
        "data_signature= (\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
        "        tf.TensorSpec(shape=(None, ), dtype=tf.int32)\n",
        ")\n",
        "#buraya bak.\n",
        "train_data = tf.data.Dataset.from_generator(\n",
        "    create_data_generator(train_dataset),\n",
        "    output_signature=data_signature\n",
        ")\n"
      ],
      "metadata": {
        "id": "M6jW1CGS7gzx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_preprocess(tokens, tag_ids):\n",
        "    preprocessed_tokens = preprocess_tokens(tokens)\n",
        "\n",
        "    # increase by 1 for all tag_ids,\n",
        "    # because `<PAD>` is added as the first element in tags list\n",
        "    preprocessed_tag_ids = tag_ids + 1\n",
        "\n",
        "    return preprocessed_tokens, preprocessed_tag_ids\n",
        "\n",
        "def preprocess_tokens(tokens):\n",
        "    tokens = tf.strings.lower(tokens)\n",
        "    return lookup_layer(tokens)\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "train_dataset = (\n",
        "    train_data.map(dataset_preprocess)\n",
        "    .padded_batch(batch_size=BATCH_SIZE).cache()\n",
        ")"
      ],
      "metadata": {
        "id": "3Qqg7nct7jXU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_bilstm_crf_model(\n",
        "    vocab_size: int, embed_dims: int, lstm_unit: int, tag_size: int\n",
        ") -> tf.keras.Model:\n",
        "    x = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"x\")\n",
        "\n",
        "    word2vec_embedding = tf.keras.layers.Embedding(input_dim=lookup_layer.vocab_size(),\n",
        "                                                   output_dim=300,\n",
        "                                                   embeddings_initializer=\"uniform\",\n",
        "                                                   mask_zero=False,\n",
        "                                                   name=\"fasttext_embedding\")\n",
        "    y = word2vec_embedding(x)\n",
        "\n",
        "    y = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(lstm_unit, return_sequences=True)\n",
        "    )(y)\n",
        "    decode_sequence, potentials, sequence_length, kernel = tfa.layers.CRF(tag_size)(y)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=x, outputs=[decode_sequence, potentials, sequence_length, kernel]\n",
        "    )"
      ],
      "metadata": {
        "id": "4qF8_80P7unM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_embedding_bilstm_crf_model(VOCAB_SIZE, 32, 64, TAG_SIZE)\n",
        "\n",
        "preprocessed_tokens = preprocess_tokens(sample_tokens)\n",
        "inputs = tf.expand_dims(preprocessed_tokens, axis=0)\n",
        "\n",
        "outputs, *_ = model(inputs)\n",
        "print(outputs[0])\n",
        "\n",
        "@tf.function\n",
        "def crf_loss_func(potentials, sequence_length, kernel, y):\n",
        "    crf_likelihood, _ = tfa.text.crf_log_likelihood(\n",
        "        potentials, y, sequence_length, kernel\n",
        "    )\n",
        "    flat_crf_loss = -1 * crf_likelihood\n",
        "    crf_loss = tf.reduce_mean(flat_crf_loss)\n",
        "\n",
        "    return crf_loss\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(0.02)\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        decoded_sequence, potentials, sequence_length, kernel = model(x)\n",
        "        crf_loss = crf_loss_func(potentials, sequence_length, kernel, y)\n",
        "        loss = crf_loss + tf.reduce_sum(model.losses)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qvGsj_gUn2h0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b03002-0ca8-4edf-d8c6-e8bd426afe0c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([3 0 2 1], shape=(4,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss.reset_states()\n",
        "\n",
        "    for x, y in train_dataset:\n",
        "        train_step(x, y)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, \" f\"Loss: {train_loss.result()}\")\n",
        "\n",
        "print(\"raw inputs: \", sample_tokens)\n",
        "\n",
        "preprocessed_inputs = preprocess_tokens(\n",
        "    sample_tokens\n",
        ")"
      ],
      "metadata": {
        "id": "ebejkzL_8-yh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a8869f0-099a-462b-94d6-a790b565f54c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4.155334949493408\n",
            "Epoch 2, Loss: 2.365445375442505\n",
            "Epoch 3, Loss: 2.043342113494873\n",
            "Epoch 4, Loss: 1.8688644170761108\n",
            "Epoch 5, Loss: 1.7537528276443481\n",
            "Epoch 6, Loss: 1.6676945686340332\n",
            "Epoch 7, Loss: 1.628190040588379\n",
            "Epoch 8, Loss: 1.5403705835342407\n",
            "Epoch 9, Loss: 1.4616292715072632\n",
            "Epoch 10, Loss: 1.4260011911392212\n",
            "raw inputs:  ['CUTTING', 'SET', 'HSS', 'PIE']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.reshape(preprocessed_inputs, shape=[1, -1])\n",
        "\n",
        "outputs, *_ = model.predict(inputs)\n",
        "prediction = [tags[i] for i in outputs[0]]\n",
        "\n",
        "print(\"ground true tags: \", sample_tags)\n",
        "print(\"predicted tags: \", prediction)"
      ],
      "metadata": {
        "id": "whZmZsno86b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f75aa7-e33a-4438-8b10-72f4468477c8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "ground true tags:  ['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT']\n",
            "predicted tags:  ['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "# Prepare test data\n",
        "test_tokens = tf.ragged.constant(test_data[\"tokens\"])\n",
        "test_tokens = tf.map_fn(tf.strings.lower, test_tokens)\n",
        "test_dataset = (\n",
        "    tf.data.Dataset.from_generator(\n",
        "        create_data_generator(test_data), output_signature=data_signature\n",
        "    )\n",
        "    .map(dataset_preprocess)\n",
        "    .padded_batch(batch_size=BATCH_SIZE)\n",
        ")\n",
        "\n",
        "# Evaluation\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for x, y in test_dataset:\n",
        "    decoded_sequence, *_ = model.predict(x)\n",
        "    y_pred.extend(decoded_sequence)\n",
        "    y_true.extend(y.numpy().tolist())\n",
        "\n",
        "# Convert tag IDs to tags\n",
        "y_true = [[tags[i] for i in seq] for seq in y_true]\n",
        "y_pred = [[tags[i] for i in seq] for seq in y_pred]\n",
        "\n",
        "# Flatten the sequences\n",
        "y_true_flat = [tag for seq in y_true for tag in seq]\n",
        "y_pred_flat = [tag for seq in y_pred for tag in seq]\n",
        "\n",
        "# Calculate metrics\n",
        "f1_score = metrics.flat_f1_score(y_true, y_pred, average='weighted', labels=tags)\n",
        "accuracy = metrics.flat_accuracy_score(y_true, y_pred)\n",
        "precision = metrics.flat_precision_score(y_true, y_pred, average='weighted', labels=tags)\n",
        "\n",
        "# Print metrics\n",
        "print(\"F1 Score:\", f1_score)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)"
      ],
      "metadata": {
        "id": "2Xxd3ziM8ivT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d47a058-a38e-4235-d030-a82cc660133b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 2s 15ms/step\n",
            "16/16 [==============================] - 0s 21ms/step\n",
            "16/16 [==============================] - 0s 21ms/step\n",
            "16/16 [==============================] - 0s 22ms/step\n",
            "16/16 [==============================] - 0s 20ms/step\n",
            "16/16 [==============================] - 0s 21ms/step\n",
            "16/16 [==============================] - 0s 24ms/step\n",
            "16/16 [==============================] - 0s 19ms/step\n",
            "16/16 [==============================] - 0s 21ms/step\n",
            "16/16 [==============================] - 0s 21ms/step\n",
            "16/16 [==============================] - 0s 20ms/step\n",
            "16/16 [==============================] - 0s 19ms/step\n",
            "16/16 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "F1 Score: 0.9060752147376019\n",
            "Accuracy: 0.9111850127480323\n",
            "Precision: 0.9083343327156123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_tokens_two = ['JEWELLERY', 'MAKING', 'TOOLS', 'WOODEN', 'PIN']\n",
        "sample_tag_two_ids = [0, 0, 0, 1, 2]\n",
        "\n",
        "sample_tags_two = [raw_tags[i] for i in sample_tag_two_ids]\n",
        "\n",
        "print(sample_tokens_two)\n",
        "print(sample_tags_two)\n",
        "\n",
        "print(\"raw inputs: \", sample_tokens_two)\n",
        "\n",
        "preprocessed_inputs_two = preprocess_tokens(\n",
        "    sample_tokens_two\n",
        ")\n",
        "# expend the batch dim\n",
        "inputs = tf.reshape(preprocessed_inputs_two, shape=[1, -1])\n",
        "\n",
        "outputs, *_ = model.predict(inputs)\n",
        "prediction = [tags[i] for i in outputs[0]]\n",
        "print(preprocessed_inputs_two)\n",
        "# Keypoint: EU -> B-ORG, German -> B-MISC, British -> B-MISC\n",
        "print(\"ground true tags: \", sample_tags_two)\n",
        "print(\"predicted tags: \", prediction)\n",
        "\n",
        "\n",
        "print(outputs)\n"
      ],
      "metadata": {
        "id": "iXbeO9Ra8vkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7393a299-cc17-45a3-e5fe-4dfa40e9752e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['JEWELLERY', 'MAKING', 'TOOLS', 'WOODEN', 'PIN']\n",
            "['O', 'O', 'O', 'B_PRODUCT', 'I_PRODUCT']\n",
            "raw inputs:  ['JEWELLERY', 'MAKING', 'TOOLS', 'WOODEN', 'PIN']\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "tf.Tensor([12731   356  1417  4714  5831], shape=(5,), dtype=int64)\n",
            "ground true tags:  ['O', 'O', 'O', 'B_PRODUCT', 'I_PRODUCT']\n",
            "predicted tags:  ['O', 'O', 'O', 'B_PRODUCT', 'I_PRODUCT']\n",
            "[[1 1 1 2 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# expend the batch dim\n",
        "\n",
        "inputs = tf.reshape(preprocessed_inputs, shape=[1, -1])\n",
        "\n",
        "outputs, *_ = model.predict(inputs)\n",
        "prediction = [tags[i] for i in outputs[0]]\n",
        "print(preprocessed_inputs)\n",
        "print(\"raw inputs: \", sample_tokens)\n",
        "# Keypoint: EU -> B-ORG, German -> B-MISC, British -> B-MISC\n",
        "print(\"ground true tags: \", sample_tags)\n",
        "print(\"predicted tags: \", prediction)\n",
        "\n",
        "\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "KTo5kfnSDwta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60f48a9c-b130-44a9-8ea8-890d79138d34"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            "tf.Tensor([3258  267    0 6508], shape=(4,), dtype=int64)\n",
            "raw inputs:  ['CUTTING', 'SET', 'HSS', 'PIE']\n",
            "ground true tags:  ['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT']\n",
            "predicted tags:  ['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT']\n",
            "[[2 3 3 3]]\n"
          ]
        }
      ]
    }
  ]
}