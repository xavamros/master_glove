{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-addons\n",
        "!pip install -q tensorflow\n",
        "!pip install -q datasets\n",
        "!pip install -q gensim\n",
        "!pip install sklearn_crfsuite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4z7y56-7Er5",
        "outputId": "6db097c9-1ced-45c0-ab08-95cf17dc0cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.10/dist-packages (0.3.6)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.9.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import datasets\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader as gensim_api\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C6jFgQm7Gmd",
        "outputId": "ddf6495f-1ea2-4fcd-a02a-5de89ee293b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data from the url\n",
        "url = \"https://drive.google.com/uc?id=1LYi4J9yquBzyVE1op9_uyFLPdo9-jq8t\"\n",
        "train_data = pd.read_csv(url, delimiter='\\t', header=None, names=['word', 'tag'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "#msk = np.random.rand(len(data)) < 0.8\n",
        "#train_data = data[msk]\n",
        "\n",
        "# Convert the words and tags into numerical representations\n",
        "word2idx = {w: i+1 for i, w in enumerate(set(train_data['word']))}\n",
        "tag2idx = {'O': 0, 'B_PRODUCT': 1, 'I_PRODUCT': 2}\n",
        "train_data['word_idx'] = train_data['word'].map(word2idx)\n",
        "train_data['tag_idx'] = train_data['tag'].map(tag2idx)\n",
        "\n",
        "# Create tokens, id, and ner_tags list from the training data\n",
        "tokens = []\n",
        "ids = []\n",
        "ner_tags = []\n",
        "temp_token_list = []\n",
        "temp_id_list = []\n",
        "temp_ner_tag_list = []\n",
        "\n",
        "# Iterate through the rows of the training data\n",
        "for index, row in train_data.iterrows():\n",
        "    # Check if the word column starts with \"|\"\n",
        "    if row['word'][0] == '|':\n",
        "        # Add the temporary lists to the main lists\n",
        "        if len(temp_token_list)>0:\n",
        "            tokens.append(temp_token_list)\n",
        "            ids.append(temp_id_list)\n",
        "            ner_tags.append(temp_ner_tag_list)\n",
        "        # Clear the temporary lists\n",
        "        temp_token_list = []\n",
        "        temp_id_list = []\n",
        "        temp_ner_tag_list = []\n",
        "    else:\n",
        "        # Add the word, tag, and tag_idx to the temporary lists\n",
        "        temp_token_list.append(row['word'])\n",
        "        temp_id_list.append(tag2idx[row['tag']])\n",
        "        temp_ner_tag_list.append(row['tag'])\n",
        "\n",
        "# Add the last example to the main lists\n",
        "if len(temp_token_list)>0:\n",
        "    tokens.append(temp_token_list)\n",
        "    ids.append(temp_id_list)\n",
        "    ner_tags.append(temp_ner_tag_list)\n",
        "\n",
        "# Split the dataset into train and test datasets\n",
        "tokens_train, tokens_test, ids_train, ids_test, ner_tags_train, ner_tags_test = train_test_split(\n",
        "    tokens, ids, ner_tags, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a dictionary with 'tokens', 'id', and 'ner_tags' keys for train and test datasets\n",
        "train_dict = {'tokens': tokens_train, 'id': ids_train, 'ner_tags': ner_tags_train}\n",
        "test_dict = {'tokens': tokens_test, 'id': ids_test, 'ner_tags': ner_tags_test}\n",
        "\n",
        "# Create Dataset objects with the train_dict and test_dict dictionaries\n",
        "train_dataset = Dataset.from_dict(train_dict)\n",
        "test_data = Dataset.from_dict(test_dict)"
      ],
      "metadata": {
        "id": "XT1pQROH7Kqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in train_dataset:\n",
        "  sample_tokens = item['tokens']\n",
        "  sample_tag_ids = item[\"id\"]\n",
        "  print(sample_tokens)\n",
        "  print(sample_tag_ids)\n",
        "  break\n",
        "\n",
        "raw_tags = ['O', 'B_PRODUCT', 'I_PRODUCT']\n",
        "print(raw_tags)\n",
        "print(type(raw_tags))\n",
        "\n",
        "sample_tags = [raw_tags[i] for i in sample_tag_ids]\n",
        "\n",
        "print(sample_tokens)\n",
        "print(sample_tags)\n",
        "\n",
        "tags = ['<PAD>'] + raw_tags\n",
        "print(tags)\n",
        "\n",
        "TAG_SIZE = len(tags)\n",
        "VOCAB_SIZE = 20000\n",
        "EMBEDDING_DIM = 300  # set the embedding dimension to 100\n",
        "\n",
        "train_tokens = tf.ragged.constant(train_dataset[\"tokens\"])\n",
        "train_tokens = tf.map_fn(tf.strings.lower, train_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oNXs6ZE7UBZ",
        "outputId": "65850688-33e4-4b5e-c46b-186ca741048f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CUTTING', 'SET', 'HSS', 'PIE']\n",
            "[1, 2, 2, 2]\n",
            "['O', 'B_PRODUCT', 'I_PRODUCT']\n",
            "<class 'list'>\n",
            "['CUTTING', 'SET', 'HSS', 'PIE']\n",
            "['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT']\n",
            "['<PAD>', 'O', 'B_PRODUCT', 'I_PRODUCT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = gensim_api.load(\"word2vec-google-news-300\")\n",
        "lookup_layer = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    #vocabulary=list(word2vec_model.index_to_key)[:20000], mask_token=None\n",
        "    vocabulary=list(word2vec_model.index_to_key), mask_token=None\n",
        ")\n",
        "embedding_matrix = np.zeros((lookup_layer.vocabulary_size(), EMBEDDING_DIM))\n",
        "for i, word in enumerate(lookup_layer.get_vocabulary()):\n",
        "    if word in word2vec_model:\n",
        "        embedding_matrix[i] = word2vec_model.get_vector(word)\n",
        "\n",
        "embedding_layer = tf.keras.layers.Embedding(\n",
        "    lookup_layer.vocabulary_size(), EMBEDDING_DIM, mask_zero=True,\n",
        "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "L_rasrvY7c8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_generator(dataset):\n",
        "  def data_generator():\n",
        "    for item in dataset:\n",
        "      yield item['tokens'], item['id']\n",
        "\n",
        "  return data_generator\n",
        "\n",
        "data_signature= (\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
        "        tf.TensorSpec(shape=(None, ), dtype=tf.int32)\n",
        ")\n",
        "#buraya bak.\n",
        "train_data = tf.data.Dataset.from_generator(\n",
        "    create_data_generator(train_dataset),\n",
        "    output_signature=data_signature\n",
        ")\n"
      ],
      "metadata": {
        "id": "M6jW1CGS7gzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_preprocess(tokens, tag_ids):\n",
        "    preprocessed_tokens = preprocess_tokens(tokens)\n",
        "\n",
        "    # increase by 1 for all tag_ids,\n",
        "    # because `<PAD>` is added as the first element in tags list\n",
        "    preprocessed_tag_ids = tag_ids + 1\n",
        "\n",
        "    return preprocessed_tokens, preprocessed_tag_ids\n",
        "\n",
        "def preprocess_tokens(tokens):\n",
        "    tokens = tf.strings.lower(tokens)\n",
        "    return lookup_layer(tokens)\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "train_dataset = (\n",
        "    train_data.map(dataset_preprocess)\n",
        "    .padded_batch(batch_size=BATCH_SIZE).cache()\n",
        ")"
      ],
      "metadata": {
        "id": "3Qqg7nct7jXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_bilstm_crf_model(\n",
        "    vocab_size: int, embed_dims: int, lstm_unit: int, tag_size: int\n",
        ") -> tf.keras.Model:\n",
        "    x = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"x\")\n",
        "\n",
        "    word2vec_embedding = tf.keras.layers.Embedding(input_dim=lookup_layer.vocab_size(),\n",
        "                                                   output_dim=300,\n",
        "                                                   embeddings_initializer=\"uniform\",\n",
        "                                                   mask_zero=False,\n",
        "                                                   name=\"word2vec_embedding\")\n",
        "    y = word2vec_embedding(x)\n",
        "\n",
        "    y = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(lstm_unit, return_sequences=True)\n",
        "    )(y)\n",
        "    decode_sequence, potentials, sequence_length, kernel = tfa.layers.CRF(tag_size)(y)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=x, outputs=[decode_sequence, potentials, sequence_length, kernel]\n",
        "    )"
      ],
      "metadata": {
        "id": "4qF8_80P7unM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_embedding_bilstm_crf_model(VOCAB_SIZE, 32, 64, TAG_SIZE)\n",
        "\n",
        "preprocessed_tokens = preprocess_tokens(sample_tokens)\n",
        "inputs = tf.expand_dims(preprocessed_tokens, axis=0)\n",
        "\n",
        "outputs, *_ = model(inputs)\n",
        "print(outputs[0])\n",
        "\n",
        "@tf.function\n",
        "def crf_loss_func(potentials, sequence_length, kernel, y):\n",
        "    crf_likelihood, _ = tfa.text.crf_log_likelihood(\n",
        "        potentials, y, sequence_length, kernel\n",
        "    )\n",
        "    flat_crf_loss = -1 * crf_likelihood\n",
        "    crf_loss = tf.reduce_mean(flat_crf_loss)\n",
        "\n",
        "    return crf_loss\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(0.02)\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        decoded_sequence, potentials, sequence_length, kernel = model(x)\n",
        "        crf_loss = crf_loss_func(potentials, sequence_length, kernel, y)\n",
        "        loss = crf_loss + tf.reduce_sum(model.losses)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qvGsj_gUn2h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss.reset_states()\n",
        "\n",
        "    for x, y in train_dataset:\n",
        "        train_step(x, y)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, \" f\"Loss: {train_loss.result()}\")\n",
        "\n",
        "print(\"raw inputs: \", sample_tokens)\n",
        "\n",
        "preprocessed_inputs = preprocess_tokens(\n",
        "    sample_tokens\n",
        ")"
      ],
      "metadata": {
        "id": "ebejkzL_8-yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.reshape(preprocessed_inputs, shape=[1, -1])\n",
        "\n",
        "outputs, *_ = model.predict(inputs)\n",
        "prediction = [tags[i] for i in outputs[0]]\n",
        "\n",
        "print(\"ground true tags: \", sample_tags)\n",
        "print(\"predicted tags: \", prediction)"
      ],
      "metadata": {
        "id": "whZmZsno86b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "# Prepare test data\n",
        "test_tokens = tf.ragged.constant(test_data[\"tokens\"])\n",
        "test_tokens = tf.map_fn(tf.strings.lower, test_tokens)\n",
        "test_dataset = (\n",
        "    tf.data.Dataset.from_generator(\n",
        "        create_data_generator(test_data), output_signature=data_signature\n",
        "    )\n",
        "    .map(dataset_preprocess)\n",
        "    .padded_batch(batch_size=BATCH_SIZE)\n",
        ")\n",
        "\n",
        "# Evaluation\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for x, y in test_dataset:\n",
        "    decoded_sequence, *_ = model.predict(x)\n",
        "    y_pred.extend(decoded_sequence)\n",
        "    y_true.extend(y.numpy().tolist())\n",
        "\n",
        "# Convert tag IDs to tags\n",
        "y_true = [[tags[i] for i in seq] for seq in y_true]\n",
        "y_pred = [[tags[i] for i in seq] for seq in y_pred]\n",
        "\n",
        "# Flatten the sequences\n",
        "y_true_flat = [tag for seq in y_true for tag in seq]\n",
        "y_pred_flat = [tag for seq in y_pred for tag in seq]\n",
        "\n",
        "# Calculate metrics\n",
        "f1_score = metrics.flat_f1_score(y_true, y_pred, average='weighted', labels=tags)\n",
        "accuracy = metrics.flat_accuracy_score(y_true, y_pred)\n",
        "precision = metrics.flat_precision_score(y_true, y_pred, average='weighted', labels=tags)\n",
        "\n",
        "# Print metrics\n",
        "print(\"F1 Score:\", f1_score)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)"
      ],
      "metadata": {
        "id": "2Xxd3ziM8ivT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_tokens_two = ['JEWELLERY', 'MAKING', 'TOOLS', 'WOODEN', 'PIN']\n",
        "sample_tag_two_ids = [0, 0, 0, 1, 2]\n",
        "\n",
        "sample_tags_two = [raw_tags[i] for i in sample_tag_two_ids]\n",
        "\n",
        "print(sample_tokens_two)\n",
        "print(sample_tags_two)\n",
        "\n",
        "print(\"raw inputs: \", sample_tokens_two)\n",
        "\n",
        "preprocessed_inputs_two = preprocess_tokens(\n",
        "    sample_tokens_two\n",
        ")\n",
        "# expend the batch dim\n",
        "inputs = tf.reshape(preprocessed_inputs_two, shape=[1, -1])\n",
        "\n",
        "outputs, *_ = model.predict(inputs)\n",
        "prediction = [tags[i] for i in outputs[0]]\n",
        "print(preprocessed_inputs_two)\n",
        "# Keypoint: EU -> B-ORG, German -> B-MISC, British -> B-MISC\n",
        "print(\"ground true tags: \", sample_tags_two)\n",
        "print(\"predicted tags: \", prediction)\n",
        "\n",
        "\n",
        "print(outputs)\n"
      ],
      "metadata": {
        "id": "iXbeO9Ra8vkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# expend the batch dim\n",
        "\n",
        "inputs = tf.reshape(preprocessed_inputs, shape=[1, -1])\n",
        "\n",
        "outputs, *_ = model.predict(inputs)\n",
        "prediction = [tags[i] for i in outputs[0]]\n",
        "print(preprocessed_inputs)\n",
        "print(\"raw inputs: \", sample_tokens)\n",
        "# Keypoint: EU -> B-ORG, German -> B-MISC, British -> B-MISC\n",
        "print(\"ground true tags: \", sample_tags)\n",
        "print(\"predicted tags: \", prediction)\n",
        "\n",
        "\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "KTo5kfnSDwta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}