{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-addons  # version >= 0.15.0 is required\n",
        "!pip install -q tensorflow\n",
        "!pip install -q datasets\n",
        "!pip install sklearn_crfsuite\n",
        "!wget -q https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3zHqA5AiisR",
        "outputId": "2f7ea2a5-446e-4a2e-a43d-5136997d95a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn_crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn_crfsuite)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.8.10)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (4.65.0)\n",
            "Installing collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.9 sklearn_crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "iFadi2Iliken",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe4946f-b60e-47c9-9878-109c1f5b9905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from the url\n",
        "url = \"https://drive.google.com/uc?id=1LYi4J9yquBzyVE1op9_uyFLPdo9-jq8t\"\n",
        "train_data = pd.read_csv(url, delimiter='\\t', header=None, names=['word', 'tag'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "#msk = np.random.rand(len(data)) < 0.8\n",
        "#train_data = data[msk]\n",
        "\n",
        "# Convert the words and tags into numerical representations\n",
        "word2idx = {w: i+1 for i, w in enumerate(set(train_data['word']))}\n",
        "tag2idx = {'O': 0, 'B_PRODUCT': 1, 'I_PRODUCT': 2}\n",
        "train_data['word_idx'] = train_data['word'].map(word2idx)\n",
        "train_data['tag_idx'] = train_data['tag'].map(tag2idx)\n",
        "\n",
        "# Create tokens, id, and ner_tags list from the training data\n",
        "tokens = []\n",
        "ids = []\n",
        "ner_tags = []\n",
        "temp_token_list = []\n",
        "temp_id_list = []\n",
        "temp_ner_tag_list = []\n",
        "\n",
        "# Iterate through the rows of the training data\n",
        "for index, row in train_data.iterrows():\n",
        "    # Check if the word column starts with \"|\"\n",
        "    if row['word'][0] == '|':\n",
        "        # Add the temporary lists to the main lists\n",
        "        if len(temp_token_list)>0:\n",
        "            tokens.append(temp_token_list)\n",
        "            ids.append(temp_id_list)\n",
        "            ner_tags.append(temp_ner_tag_list)\n",
        "        # Clear the temporary lists\n",
        "        temp_token_list = []\n",
        "        temp_id_list = []\n",
        "        temp_ner_tag_list = []\n",
        "    else:\n",
        "        # Add the word, tag, and tag_idx to the temporary lists\n",
        "        temp_token_list.append(row['word'])\n",
        "        temp_id_list.append(tag2idx[row['tag']])\n",
        "        temp_ner_tag_list.append(row['tag'])\n",
        "\n",
        "# Add the last example to the main lists\n",
        "if len(temp_token_list)>0:\n",
        "    tokens.append(temp_token_list)\n",
        "    ids.append(temp_id_list)\n",
        "    ner_tags.append(temp_ner_tag_list)\n",
        "\n",
        "# Split the dataset into train and test datasets\n",
        "tokens_train, tokens_test, ids_train, ids_test, ner_tags_train, ner_tags_test = train_test_split(\n",
        "    tokens, ids, ner_tags, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a dictionary with 'tokens', 'id', and 'ner_tags' keys for train and test datasets\n",
        "train_dict = {'tokens': tokens_train, 'id': ids_train, 'ner_tags': ner_tags_train}\n",
        "test_dict = {'tokens': tokens_test, 'id': ids_test, 'ner_tags': ner_tags_test}\n",
        "\n",
        "# Create Dataset objects with the train_dict and test_dict dictionaries\n",
        "train_dataset = Dataset.from_dict(train_dict)\n",
        "test_data = Dataset.from_dict(test_dict)"
      ],
      "metadata": {
        "id": "8X6jrCqriryn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the dataset\n",
        "print(train_dataset)\n",
        "print(train_dataset['tokens'][0])\n",
        "print(train_dataset['id'][0])\n",
        "print(train_dataset['ner_tags'][0])\n",
        "\n",
        "# Print the dataset\n",
        "#print(train_dataset)\n",
        "print(train_dataset['tokens'][:20])\n",
        "print(train_dataset['id'][:20])\n",
        "print(train_dataset['ner_tags'][:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU2ZcBgqCCZR",
        "outputId": "bacc161f-bdf9-4f42-df9a-585f19d9da63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['tokens', 'id', 'ner_tags'],\n",
            "    num_rows: 26868\n",
            "})\n",
            "['CUTTING', 'SET', 'HSS', 'PIE']\n",
            "[1, 2, 2, 2]\n",
            "['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT']\n",
            "[['CUTTING', 'SET', 'HSS', 'PIE'], ['JEWELLERY', 'MAKING', 'TOOLS', 'BRASS', 'SHUNK', 'PIN'], ['ETG', 'TOOLS'], ['CARBIDE', 'INSERT', 'CLASS', 'CLASS'], ['UOCHIKYU', 'NEEDLE', 'FILE', 'FLAT', 'FOR', 'MFG', 'PURPOSE'], ['PARCEL', 'PAPER', 'WHITE', 'WHITE'], ['INDUSTRIAL', 'ENGINEERING', 'TOOLS', 'PRECISION'], ['NONMAG', 'CRANKED', 'SOCKET', 'WRENCH'], ['CRUSHER', 'MACHINE', 'SPARES', 'VALVE', 'INSERT'], ['KOL', 'DRILL', 'BIT', 'TOOL', 'HAND', 'TOOLS'], ['HAND', 'TOOLS', 'MICRO', 'FLUSH', 'CUTTING', 'PLIER'], ['CONE', 'OUTLET', 'UPR', 'TGDI'], ['TOOLS', 'CARTRIDGES'], ['ROTARY', 'FILERS', 'HARDWARE', 'TOOLS'], ['HEDGE', 'SHEAR', 'ULTRA', 'TOUCH'], ['DIE', 'CUTTING', 'BLADE', 'DTL', 'SPECIAL', 'LCB', 'ACW', 'COIL', 'COIL', 'ACW'], ['SYNTEK', 'DOCTOR', 'BLADE', 'LENGTH', 'WIDTH', 'THICK'], ['CLAMP', 'FOR', 'MACHINE', 'TOOL', 'PARTS'], ['USED', 'RECOIL', 'INSERT', 'YOM', 'OF', 'TOOLING', 'FOR', 'TURBI'], ['HAND', 'TOOLS', 'COMBINATION', 'SPANNER', 'NON', 'ALLO']]\n",
            "[[1, 2, 2, 2], [0, 0, 0, 1, 0, 2], [1, 2], [1, 2, 2, 2], [0, 2, 2, 1, 0, 0, 0], [1, 2, 2, 2], [1, 0, 2, 0], [0, 1, 2, 2], [0, 0, 0, 1, 2], [0, 1, 2, 2, 0, 0], [0, 0, 1, 2, 2, 2], [1, 2, 0, 0], [1, 2], [1, 2, 2, 2], [1, 2, 0, 0], [1, 2, 2, 2, 2, 2, 2, 2, 2, 2], [1, 2, 2, 2, 2, 2], [1, 0, 0, 0, 0], [1, 2, 2, 0, 0, 0, 0, 0], [0, 0, 1, 2, 0, 0]]\n",
            "[['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT'], ['O', 'O', 'O', 'B_PRODUCT', 'O', 'I_PRODUCT'], ['B_PRODUCT', 'I_PRODUCT'], ['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT'], ['O', 'I_PRODUCT', 'I_PRODUCT', 'B_PRODUCT', 'O', 'O', 'O'], ['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT'], ['B_PRODUCT', 'O', 'I_PRODUCT', 'O'], ['O', 'B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT'], ['O', 'O', 'O', 'B_PRODUCT', 'I_PRODUCT'], ['O', 'B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'O', 'O'], ['O', 'O', 'B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT'], ['B_PRODUCT', 'I_PRODUCT', 'O', 'O'], ['B_PRODUCT', 'I_PRODUCT'], ['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT'], ['B_PRODUCT', 'I_PRODUCT', 'O', 'O'], ['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT'], ['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT'], ['B_PRODUCT', 'O', 'O', 'O', 'O'], ['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B_PRODUCT', 'I_PRODUCT', 'O', 'O']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in train_dataset:\n",
        "  sample_tokens = item['tokens']\n",
        "  sample_tag_ids = item[\"id\"]\n",
        "  print(sample_tokens)\n",
        "  print(sample_tag_ids)\n",
        "  break\n",
        "\n",
        "raw_tags = ['O', 'B_PRODUCT', 'I_PRODUCT']\n",
        "print(raw_tags)\n",
        "print(type(raw_tags))\n",
        "\n",
        "sample_tags = [raw_tags[i] for i in sample_tag_ids]\n",
        "\n",
        "print(sample_tokens)\n",
        "print(sample_tags)\n",
        "\n",
        "tags = ['<PAD>'] + raw_tags\n",
        "print(tags)\n",
        "\n",
        "TAG_SIZE = len(tags)\n",
        "VOCAB_SIZE = 20000\n",
        "EMBEDDING_DIM = 100  # set the embedding dimension to 100\n",
        "\n",
        "train_tokens = tf.ragged.constant(train_dataset[\"tokens\"])\n",
        "train_tokens = tf.map_fn(tf.strings.lower, train_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeS4-Ig6ivhE",
        "outputId": "d231a7dd-fbac-456c-f57e-bbdf823069ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CUTTING', 'SET', 'HSS', 'PIE']\n",
            "[1, 2, 2, 2]\n",
            "['O', 'B_PRODUCT', 'I_PRODUCT']\n",
            "<class 'list'>\n",
            "['CUTTING', 'SET', 'HSS', 'PIE']\n",
            "['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT']\n",
            "['<PAD>', 'O', 'B_PRODUCT', 'I_PRODUCT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the GloVe embeddings file into a dictionary\n",
        "glove_file = \"glove.6B.100d.txt\"\n",
        "word_to_embedding = {}\n",
        "with open(glove_file, encoding=\"utf8\", mode= \"r\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype=\"float32\")\n",
        "        word_to_embedding[word] = embedding"
      ],
      "metadata": {
        "id": "hR1kXm_Oiyu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookup_layer = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=list(word_to_embedding.keys()), mask_token=None\n",
        ")\n",
        "embedding_matrix = np.zeros((lookup_layer.vocabulary_size(), EMBEDDING_DIM))\n",
        "for i, word in enumerate(lookup_layer.get_vocabulary()):\n",
        "    if word in word_to_embedding:\n",
        "        embedding_matrix[i] = word_to_embedding[word]\n",
        "\n",
        "embedding_layer = tf.keras.layers.Embedding(\n",
        "    lookup_layer.vocabulary_size(), EMBEDDING_DIM, mask_zero=True,\n",
        "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False  # set trainable to False to use the pre-trained embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "pdxTNtRmi1gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_generator(dataset):\n",
        "  def data_generator():\n",
        "    for item in dataset:\n",
        "      yield item['tokens'], item['id']\n",
        "\n",
        "  return data_generator\n",
        "\n",
        "data_signature= (\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
        "        tf.TensorSpec(shape=(None, ), dtype=tf.int32)\n",
        ")\n",
        "#buraya bak.\n",
        "train_data = tf.data.Dataset.from_generator(\n",
        "    create_data_generator(train_dataset),\n",
        "    output_signature=data_signature\n",
        ")"
      ],
      "metadata": {
        "id": "dOXDnpqmi7sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_preprocess(tokens, tag_ids):\n",
        "    preprocessed_tokens = preprecess_tokens(tokens)\n",
        "\n",
        "    # increase by 1 for all tag_ids,\n",
        "    # because `<PAD>` is added as the first element in tags list\n",
        "    preprocessed_tag_ids = tag_ids + 1\n",
        "\n",
        "    return preprocessed_tokens, preprocessed_tag_ids\n",
        "\n",
        "\n",
        "def preprecess_tokens(tokens):\n",
        "    tokens = tf.strings.lower(tokens)\n",
        "    return lookup_layer(tokens)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 2048\n",
        "\n",
        "# With `padded_batch()`, each batch may have different length\n",
        "# shape: (batch_size, None)\n",
        "train_dataset = (\n",
        "    train_data.map(dataset_preprocess)\n",
        "    .padded_batch(batch_size=BATCH_SIZE).cache()\n",
        ")"
      ],
      "metadata": {
        "id": "YkS3KqSLi95h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_bilstm_crf_model(\n",
        "    vocab_size: int, embed_dims: int, lstm_unit: int, tag_size: int\n",
        ") -> tf.keras.Model:\n",
        "    x = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"x\")\n",
        "\n",
        "    # Replace the Embedding layer with pre-trained GloVe embedding\n",
        "    glove_embedding = tf.keras.layers.Embedding(input_dim=lookup_layer.vocab_size(),\n",
        "                                                output_dim=100,\n",
        "                                                weights=[embedding_matrix],\n",
        "                                                trainable=False,\n",
        "                                                mask_zero=True,\n",
        "                                                name=\"glove_embedding\")\n",
        "    y = glove_embedding(x)\n",
        "\n",
        "    y = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(lstm_unit, return_sequences=True)\n",
        "    )(y)\n",
        "    decode_sequence, potentials, sequence_length, kernel = tfa.layers.CRF(tag_size)(y)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=x, outputs=[decode_sequence, potentials, sequence_length, kernel]\n",
        "    )\n"
      ],
      "metadata": {
        "id": "R2LBFMyVjBY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_embedding_bilstm_crf_model(VOCAB_SIZE, 32, 64, TAG_SIZE)\n",
        "\n",
        "# preprocess\n",
        "preprecessd_tokens = preprecess_tokens(sample_tokens)\n",
        "\n",
        "# expand the tensor to shape: [1, None]. That is add batch dim\n",
        "inputs = tf.expand_dims(preprecessd_tokens, axis=0)\n",
        "\n",
        "outputs, *_ = model(inputs)\n",
        "print(outputs[0])\n",
        "\n",
        "@tf.function\n",
        "def crf_loss_func(potentials, sequence_length, kernel, y):\n",
        "    crf_likelihood, _ = tfa.text.crf_log_likelihood(\n",
        "        potentials, y, sequence_length, kernel\n",
        "    )\n",
        "    # likelihood to loss\n",
        "    flat_crf_loss = -1 * crf_likelihood\n",
        "    crf_loss = tf.reduce_mean(flat_crf_loss)\n",
        "\n",
        "    return crf_loss\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(0.02)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        decoded_sequence, potentials, sequence_length, kernel = model(x)\n",
        "        crf_loss = crf_loss_func(potentials, sequence_length, kernel, y)\n",
        "        loss = crf_loss + tf.reduce_sum(model.losses)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVsiNXJ7jGdl",
        "outputId": "b19f83ed-9e1d-488c-d7df-8d5f90aa787e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1 2 3 0], shape=(4,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEnbfCqCics7",
        "outputId": "6090215b-746a-4715-9319-0ff3d508ba0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3.8536198139190674\n",
            "Epoch 2, Loss: 2.547311544418335\n",
            "Epoch 3, Loss: 2.0895702838897705\n",
            "Epoch 4, Loss: 1.8242772817611694\n",
            "Epoch 5, Loss: 1.6348438262939453\n",
            "Epoch 6, Loss: 1.4890105724334717\n",
            "Epoch 7, Loss: 1.3604776859283447\n",
            "Epoch 8, Loss: 1.2493884563446045\n",
            "Epoch 9, Loss: 1.1517715454101562\n",
            "Epoch 10, Loss: 1.0663516521453857\n",
            "raw inputs:  ['CUTTING', 'SET', 'HSS', 'PIE']\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "\n",
        "    for x, y in train_dataset:\n",
        "        train_step(x, y)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, \" f\"Loss: {train_loss.result()}\")\n",
        "\n",
        "\n",
        "# print the inputs and expected outputs\n",
        "print(\"raw inputs: \", sample_tokens)\n",
        "\n",
        "# preprocess\n",
        "preprocessed_inputs = preprecess_tokens(\n",
        "    sample_tokens\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "# Prepare test data\n",
        "test_tokens = tf.ragged.constant(test_data[\"tokens\"])\n",
        "test_tokens = tf.map_fn(tf.strings.lower, test_tokens)\n",
        "test_dataset = (\n",
        "    tf.data.Dataset.from_generator(\n",
        "        create_data_generator(test_data), output_signature=data_signature\n",
        "    )\n",
        "    .map(dataset_preprocess)\n",
        "    .padded_batch(batch_size=BATCH_SIZE)\n",
        ")\n",
        "\n",
        "# Evaluation\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for x, y in test_dataset:\n",
        "    decoded_sequence, *_ = model.predict(x)\n",
        "    y_pred.extend(decoded_sequence)\n",
        "    y_true.extend(y.numpy().tolist())\n",
        "\n",
        "# Convert tag IDs to tags\n",
        "y_true = [[tags[i] for i in seq] for seq in y_true]\n",
        "y_pred = [[tags[i] for i in seq] for seq in y_pred]\n",
        "\n",
        "# Flatten the sequences\n",
        "y_true_flat = [tag for seq in y_true for tag in seq]\n",
        "y_pred_flat = [tag for seq in y_pred for tag in seq]\n",
        "\n",
        "# Calculate metrics\n",
        "f1_score = metrics.flat_f1_score(y_true, y_pred, average='weighted', labels=tags)\n",
        "accuracy = metrics.flat_accuracy_score(y_true, y_pred)\n",
        "precision = metrics.flat_precision_score(y_true, y_pred, average='weighted', labels=tags)\n",
        "\n",
        "# Print metrics\n",
        "print(\"F1 Score:\", f1_score)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSmNpFL0BF-3",
        "outputId": "6cd7b020-b44b-4c01-a0e9-14f29446a4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64/64 [==============================] - 4s 11ms/step\n",
            "64/64 [==============================] - 4s 10ms/step\n",
            "64/64 [==============================] - 1s 11ms/step\n",
            "18/18 [==============================] - 3s 9ms/step\n",
            "F1 Score: 0.9136733917079655\n",
            "Accuracy: 0.9163614708947446\n",
            "Precision: 0.9124278728917394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_tokens_two = ['JEWELLERY', 'MAKING', 'TOOLS', 'WOODEN', 'PIN']\n",
        "sample_tag_two_ids = [0, 0, 0, 1, 2]\n",
        "\n",
        "sample_tags_two = [raw_tags[i] for i in sample_tag_two_ids]\n",
        "\n",
        "print(sample_tokens_two)\n",
        "print(sample_tags_two)\n",
        "\n",
        "print(\"raw inputs: \", sample_tokens_two)\n",
        "\n",
        "preprocessed_inputs_two = preprecess_tokens(\n",
        "    sample_tokens_two\n",
        ")\n",
        "# expend the batch dim\n",
        "inputs = tf.reshape(preprocessed_inputs_two, shape=[1, -1])\n",
        "\n",
        "outputs, *_ = model.predict(inputs)\n",
        "prediction = [tags[i] for i in outputs[0]]\n",
        "print(preprocessed_inputs_two)\n",
        "# Keypoint: EU -> B-ORG, German -> B-MISC, British -> B-MISC\n",
        "print(\"ground true tags: \", sample_tags_two)\n",
        "print(\"predicted tags: \", prediction)\n",
        "\n",
        "\n",
        "print(outputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1mHtXZ7mddw",
        "outputId": "6efd252a-442c-46cd-b1be-a4f2f5bed965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['JEWELLERY', 'MAKING', 'TOOLS', 'WOODEN', 'PIN']\n",
            "['O', 'O', 'O', 'B_PRODUCT', 'I_PRODUCT']\n",
            "raw inputs:  ['JEWELLERY', 'MAKING', 'TOOLS', 'WOODEN', 'PIN']\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "tf.Tensor([21975   434  4316  4837  8427], shape=(5,), dtype=int64)\n",
            "ground true tags:  ['O', 'O', 'O', 'B_PRODUCT', 'I_PRODUCT']\n",
            "predicted tags:  ['O', 'O', 'O', 'B_PRODUCT', 'I_PRODUCT']\n",
            "[[1 1 1 2 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qpRY1FzxnH4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# expend the batch dim\n",
        "\n",
        "inputs = tf.reshape(preprocessed_inputs, shape=[1, -1])\n",
        "\n",
        "outputs, *_ = model.predict(inputs)\n",
        "prediction = [tags[i] for i in outputs[0]]\n",
        "print(preprocessed_inputs)\n",
        "print(\"raw inputs: \", sample_tokens)\n",
        "# Keypoint: EU -> B-ORG, German -> B-MISC, British -> B-MISC\n",
        "print(\"ground true tags: \", sample_tags)\n",
        "print(\"predicted tags: \", prediction)\n",
        "\n",
        "\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-XnQtqZjI2j",
        "outputId": "7fcd80db-ec3c-4041-f55a-f602fef678ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n",
            "tf.Tensor([  2579    209 103169   9247], shape=(4,), dtype=int64)\n",
            "raw inputs:  ['CUTTING', 'SET', 'HSS', 'PIE']\n",
            "ground true tags:  ['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT']\n",
            "predicted tags:  ['B_PRODUCT', 'I_PRODUCT', 'I_PRODUCT', 'I_PRODUCT']\n",
            "[[2 3 3 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Amazon azure icin amazon ile gorusulecek. sanal makine kullanarak fasttext yapabilir miyiz.\n",
        "Bert embedding ile nasil yapariz bir de bunu deneyecegim.\n",
        "19 Haziran haftasi tekrar goruselim. En kotu Eylulde 3 ay uzatma talep ederek Aralikta bitirebiliriz, ama hedef eylulde bitirmek."
      ],
      "metadata": {
        "id": "bo3ITllik808"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}